% Encoding: UTF-8

@article{Caux2009,
	Abstract = {Recent developments in the theory of integrable models have provided the means of calculating dynamical correlation functions of some important observables in systems such as Heisenberg spin chains and one-dimensional atomic gases. This article explicitly describes how such calculations are generally implemented in the ABACUS C++ library, emphasizing the universality in treatment of different cases coming as a consequence of unifying features within the Bethe Ansatz.},
	Author = {J.-S. Caux},
	Date = {2009-08-12},
	Doi = {10.1063/1.3216474},
	Eprint = {0908.1660v1},
	Eprintclass = {cond-mat.str-el},
	Eprinttype = {arXiv},
	File = {online:http\://arxiv.org/pdf/0908.1660v1:PDF},
	Journal = {J. Math. Phys.},
	Journaltitle = {J. Math. Phys. 50, 095214 (2009)},
	Keywords = {cond-mat.str-el, cond-mat.quant-gas},
	Owner = {teun},
	Pages = {095214},
	Title = {{Correlation functions of integrable models: a description of the ABACUS algorithm}},
	Volume = {50},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1063/1.3216474}}

@article{Saito2017,
	Abstract = {Motivated by the recent successful application of artificial neural networks to quantum many-body problems [G. Carleo and M. Troyer, Science {\bf 355}, 602 (2017)], a method to calculate the ground state of the Bose-Hubbard model using a feedforward neural network is proposed. The results are in good agreement with those obtained by exact diagonalization and the Gutzwiller approximation. The method of neural-network quantum states is promising for solving quantum many-body problems of ultracold atoms in optical lattices.},
	Author = {Hiroki Saito},
	Doi = {10.7566/jpsj.86.093001},
	Journal = {J. Phys. Soc. Jpn.},
	Pages = {093001},
	Title = {Solving the Bose-Hubbard model with machine learning},
	Volume = {86},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.7566/jpsj.86.093001}}

@article{Piroli2015,
	Author = {Lorenzo Piroli and Pasquale Calabrese},
	Doi = {10.1088/1751-8113/48/45/454002},
	Journal = {J. Phys. A: Math. Theor.},
	Pages = {454002},
	Title = {{Exact formulas for the form factors of local operators in the Lieb--Liniger model}},
	Volume = {48},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1751-8113/48/45/454002}}

@article{Caux2007,
	Author = {Jean-S{\'e}bastien Caux and Pasquale Calabrese and Nikita A Slavnov},
	Doi = {10.1088/1742-5468/2007/01/P01008},
	Journal = {J. Stat. Mech.},
	Pages = {P01008},
	Title = {{One-particle dynamical correlations in the one-dimensional Bose gas}},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-5468/2007/01/P01008}}

@article{Caux2007a,
	Author = {Jean-S{\'e}bastien Caux and Pasquale Calabrese},
	Doi = {10.1103/.74.031605},
	Journal = {Phys. Rev. A},
	Pages = {031605(R)},
	Title = {{Dynamical density-density correlations in the one-dimensional Bose gas}},
	Volume = {74},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevA.74.031605}}

@article{Griffiths1993,
	Author = {David J. Griffiths},
	Doi = {10.1088/0305-4470/26/9/021},
	Journal = {J. Phys. A: Math. Gen.},
	Pages = {2265},
	Title = {Boundary conditions at the derivative of a delta function},
	Volume = {26},
	Year = {1993},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/0305-4470/26/9/021}}

@article{Bethe1931,
	Author = {Hans Bethe},
	Doi = {10.1007/BF01341708},
	Journal = {Z. Phys.},
	Pages = {205},
	Title = {{Zur Theorie der Metalle I. Eigenwerte und Eigenfunktionen der linearen Atomkette}},
	Volume = {71},
	Year = {1931},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/BF01341708}}

@article{Lieb1963,
	Author = {Elliott H. Lieb and Werner Liniger},
	Doi = {10.1103/PhysRev.130.1605},
	Journal = {Phys. Rev.},
	Pages = {1605},
	Title = {{Exact Analysis of an Interacting Bose Gas. I. The General Solution and the Ground State}},
	Volume = {130},
	Year = {1963},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRev.130.1605}}

@article{Lieb1963a,
	Author = {Elliott H. Lieb},
	Doi = {10.1103/PhysRev.130.1616},
	Journal = {Phys. Rev.},
	Pages = {1616},
	Title = {{Exact Analysis of an Interacting Bose Gas. II. The Excitation Spectrum}},
	Volume = {130},
	Year = {1963},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRev.130.1616}}

@article{Carleo2017,
	Author = {Giuseppe Carleo and Matthias Troyer},
	Doi = {10.1126/science.aag2302},
	Journal = {Science},
	Pages = {602},
	Title = {Solving the quantum many-body problem with artificial neural networks},
	Volume = {355},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1126/science.aag2302}}

@book{Sutherland2004,
	Author = {Bill Sutherland},
	Isbn = {978-981-238-859-9},
	Publisher = {World Scientific},
	Title = {Beautiful models: 70 years of exactly solved quantum many-body problems},
	Year = {2004}}

@book{Franchini2017,
	Author = {Fabio Franchini},
	Doi = {10.1007/978-3-319-48487-7},
	Publisher = {Springer},
	Series = {Lecture Notes in Physics},
	Title = {An Introduction to Integrable Techniques for One-Dimensional Quantum Systems},
	Volume = {940},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-3-319-48487-7}}

@article{Cazalilla2011,
	Author = {M. A. Cazalilla and R. Citro and T. Giamarchi and E. Orignac and M. Rigol},
	Doi = {10.1103/RevModPhys.83.1405},
	Journal = {Rev. Mod. Phys.},
	Pages = {1405},
	Title = {One dimensional bosons: From condensed matter systems to ultracold gases},
	Volume = {83},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/RevModPhys.83.1405}}

@unpublished{Caux2015,
	Author = {Jean-S{\'e}bastien Caux},
	Month = jun,
	Note = {Lectures for S{\'e}minaire d'excellence en physique, Paris, June 2015},
	Title = {Integrability in atomic and condensed matter physics in and out of equilibrium},
	Year = {2015}}

@article{Yang1969,
	Author = {C. N. Yang and C. P. Yang},
	Doi = {10.1063/1.1664947},
	Journal = {Journal of Mathematical Physics},
	Pages = {1115},
	Title = {Thermodynamics of a One-Dimensional System of Bosons with Repulsive Delta-Function Interaction},
	Volume = {10},
	Year = {1969},
	Bdsk-Url-1 = {http://dx.doi.org/10.1063/1.1664947}}

@article{Panfil2014,
	Author = {Mi{\l}osz Panfil and Jean-S{\'e}bastien Caux},
	Doi = {10.1103/PhysRevA.89.033605},
	Journal = {Physical Review A},
	Pages = {033605},
	Title = {Finite-temperature correlations in the Lieb-Liniger one-dimensional Bose gas},
	Volume = {89},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevA.89.033605}}

@article{Gu2005,
	Author = {S.-J. Gu and N.M.R. Peres and Y.-Q. Li},
	Doi = {10.1140/epjb/e2005-00390-1},
	Journal = {Eur. Phys. J. B},
	Pages = {157},
	Title = {Numerical and Monte Carlo Bethe ansatz method: 1D Heisenberg model},
	Volume = {48},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1140/epjb/e2005-00390-1}}

@article{Nardis2015,
	Author = {J. De Nardis and M. Panfil},
	Doi = {10.1088/1742-5468/2015/02/P02019},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Number = {P02019},
	Title = {Density form factors of the 1D Bose gas for finite entropy states},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-5468/2015/02/P02019}}

@article{Nardis2016,
	Author = {Jacopo De Nardis and Mi{\l}osz Panfil},
	Doi = {10.21468/SciPostPhys.1.2.015},
	Journal = {SciPost Phys.},
	Number = {015},
	Title = {Exact correlations in the Lieb-Liniger model and detailed balance out-of-equilibrium},
	Volume = {1},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.21468/SciPostPhys.1.2.015}}

@book{Russell2010,
	Author = {Stuart Russell and Peter Norvig},
	Edition = {3rd edition},
	Publisher = {Pearson},
	Title = {Artificial Intelligence: A Modern Approach},
	Year = {2010}}

@book{Press2007,
	Author = {William H. Press and Saul A. Teukolsky and William T. Vetterling and Brian P. Flannery},
	Publisher = {Cambridge University Press},
	Title = {Numerical Recipes: The Art of Scientific Computing},
	Year = {2007}}

@article{Fabbri2015,
	Author = {N. Fabbri and M. Panfil and D. Cl{\'e}ment and L. Fallani and M. Inguscio and C. Fort and J.-S. Caux},
	Doi = {10.1103/PhysRevA.91.043617},
	Journal = {Physical Review A},
	Number = {043617},
	Title = {Dynamical structure factor of one-dimensional Bose gases: Experimental signatures of beyond-Luttinger-liquid physics},
	Volume = {91},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevA.91.043617}}

@article{Tongeren2016,
	Author = {Stijn J. van Tongeren},
	Keywords = {arXiv:1606.02951v3},
	Title = {Introduction to the thermodynamic Bethe ansatz},
	Year = {2016}}

@Unpublished{Liu2017,
  author = {Zhaocheng Liu and Sean P. Rodrigues and Wenshan Cai},
  title  = {Simulating the Ising Model with a Deep Convolutional Generative Adversarial Network},
  note   = {arXiv:1710.04987},
}

@article{Lang2017,
	Author = {Guillaume Lang and Frank Hekking and and Anna Minguzzi},
	Doi = {10.21468/SciPostPhys.3.1.003},
	Journal = {SciPost Phys.},
	Number = {001},
	Title = {Ground-state energy and excitation spectrum of the Lieb-Liniger model: accurate analytical results and conjectures about the exact solution},
	Volume = {3},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.21468/SciPostPhys.3.1.003}}

@book{Zemyan2012,
	Author = {S. M. Zemyan},
	Doi = {10.1007/978-0-8176-8349-8_2},
	Publisher = {Springer Science},
	Title = {The Classical Theory of Integral Equations: A Concise Treatment},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-0-8176-8349-8_2}}

@misc{tofind,
	Title = {{\color{red} find references}}}

@book{Sueli2003,
	Author = {Endre S{\"u}li and David Mayers},
	Isbn = {9780521007948},
	Publisher = {Cambridge University Press},
	Title = {An introduction to numerical analysis},
	Year = {2003}}

@misc{tensorflow2015-whitepaper,
	Author = {Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dan~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
	Note = {Software available from tensorflow.org},
	Title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	Url = {https://www.tensorflow.org/},
	Year = {2015},
	Bdsk-Url-1 = {https://www.tensorflow.org/}}

@article{Radford2015,
	Abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	Author = {Alec Radford and Luke Metz and Soumith Chintala},
	Date = {2015-11-19},
	Eprint = {1511.06434v2},
	Eprintclass = {cs.LG},
	Eprinttype = {arXiv},
	File = {online:http\://arxiv.org/pdf/1511.06434v2:PDF},
	Keywords = {cs.LG, cs.CV},
	Title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}}

@article{Carrasquilla2017,
	Author = {Juan Carrasquilla and Roger G. Melko},
	Doi = {10.1038/nphys4035},
	Journal = {Nature Physics},
	Month = {feb},
	Number = {5},
	Pages = {431--434},
	Publisher = {Springer Nature},
	Title = {Machine learning phases of matter},
	Volume = {13},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1038/nphys4035}}

@article{Xia2007,
	Author = {Youshen Xia and Gang Feng},
	Doi = {10.1016/j.neunet.2007.01.001},
	Journal = {Neural Networks},
	Month = {jul},
	Number = {5},
	Pages = {577--589},
	Publisher = {Elsevier {BV}},
	Title = {A new neural network for solving nonlinear projection equations},
	Volume = {20},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neunet.2007.01.001}}

@article{Mathia1995,
	Author = {Karl Mathia and Richard Saeks},
	Journal = {World Congress on Neural Networks},
	Title = {Solving Nonlinear Equations Using Recurrent Neural Networks},
	Year = {1995}}

@article{Caux2016,
	Author = {Jean-S{\'{e}}bastien Caux},
	Doi = {10.1088/1742-5468/2016/06/064006},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Month = {jun},
	Number = {6},
	Pages = {064006},
	Publisher = {{IOP} Publishing},
	Title = {The Quench Action},
	Volume = {2016},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-5468/2016/06/064006}}

@standard{cppstandard2016,
	Institution = {International Organization for Standardization},
	Organization = {INCITS/ISO/IEC 14882:2014},
	Title = {Standard for Programming Language C++},
	Year = {2016}}

@misc{ieeefp2008,
	Doi = {10.1109/ieeestd.2008.4610935},
	Publisher = {{IEEE}},
	Title = {754-2008 - {IEEE} Standard for Floating-Point Arithmetic},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ieeestd.2008.4610935}}

@article{Nik2009,
	Author = {N. M. A. Nik and Zainidin Eshkuvatov and Mohammad Yaghobifar and M. Hasan},
	Booktitle = {International Journal of Computational and Mathematical Sciences},
	Month = {01},
	Title = {Numerical Solution of Infinite Boundary Integral Equation by using Galerkin Method with Laguerre Polynomials},
	Volume = {3},
	Year = {2009}}

@article{Kivinen2004,
	Author = {J. Kivinen and A.J. Smola and R.C. Williamson},
	Doi = {10.1109/tsp.2004.830991},
	Journal = {{IEEE} Transactions on Signal Processing},
	Month = {aug},
	Number = {8},
	Pages = {2165--2176},
	Publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	Title = {Online Learning with Kernels},
	Volume = {52},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/tsp.2004.830991}}

@article{Dunjko2017,
	Abstract = {Quantum information technologies, and intelligent learning systems, are both emergent technologies that will likely have a transforming impact on our society. The respective underlying fields of research -- quantum information (QI) versus machine learning (ML) and artificial intelligence (AI) -- have their own specific challenges, which have hitherto been investigated largely independently. However, in a growing body of recent work, researchers have been probing the question to what extent these fields can learn and benefit from each other. QML explores the interaction between quantum computing and ML, investigating how results and techniques from one field can be used to solve the problems of the other. Recently, we have witnessed breakthroughs in both directions of influence. For instance, quantum computing is finding a vital application in providing speed-ups in ML, critical in our "big data" world. Conversely, ML already permeates cutting-edge technologies, and may become instrumental in advanced quantum technologies. Aside from quantum speed-up in data analysis, or classical ML optimization used in quantum experiments, quantum enhancements have also been demonstrated for interactive learning, highlighting the potential of quantum-enhanced learning agents. Finally, works exploring the use of AI for the very design of quantum experiments, and for performing parts of genuine research autonomously, have reported their first successes. Beyond the topics of mutual enhancement, researchers have also broached the fundamental issue of quantum generalizations of ML/AI concepts. This deals with questions of the very meaning of learning and intelligence in a world that is described by quantum mechanics. In this review, we describe the main ideas, recent developments, and progress in a broad spectrum of research investigating machine learning and artificial intelligence in the quantum domain.},
	Author = {Vedran Dunjko and Hans J. Briegel},
	Date = {2017-09-08},
	Eprint = {1709.02779v1},
	Eprintclass = {quant-ph},
	Eprinttype = {arXiv},
	File = {online:http\://arxiv.org/pdf/1709.02779v1:PDF},
	Keywords = {quant-ph, cs.AI, cs.CV},
	Title = {Machine learning \& artificial intelligence in the quantum domain}}

@Book{Korepin1993,
  author =    {V. E. Korepin and N. M. Bogoliobov and A. G. Izergin},
  title =        {Quantum inverse scattering method and correlation functions},
  publisher =    {Cambridge University Press},
  year =         {1993},
}

@article{Cai2017,
	Abstract = {In this paper, we demonstrate the expressibility of artificial neural networks (ANNs) in quantum many-body physics by showing that a feed-forward neural network with a small number of hidden layers can be trained to approximate with high precision the ground states of some notable quantum many-body systems. We consider the one-dimensional free bosons and fermions, spinless fermions on a square lattice away from half-filling, as well as frustrated quantum magnetism with a rapidly oscillating ground-state characteristic function. In the latter case, an ANN with a standard architecture fails, while that with a slightly modified one successfully learns the frustration-driven complex sign rule in the ground state. The practical application of this method to explore the unknown ground states is also discussed.},
	Author = {Zi Cai},
	Date = {2017-04-17},
	Eprint = {1704.05148v3},
	Eprintclass = {cond-mat.str-el},
	Eprinttype = {arXiv},
	File = {online:http\://arxiv.org/pdf/1704.05148v3:PDF},
	Keywords = {cond-mat.str-el},
	Title = {Approximating quantum many-body wave-functions using artificial neural networks}}

@article{Silver2017,
	Abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
	Author = {David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
	Date = {2017-12-05},
	Eprint = {1712.01815v1},
	Eprintclass = {cs.AI},
	Eprinttype = {arXiv},
	File = {online:http\://arxiv.org/pdf/1712.01815v1:PDF},
	Keywords = {cs.AI, cs.LG},
	Title = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}}

@article{Poplin2016,
	Author = {Ryan Poplin and Dan Newburger and Jojo Dijamco and Nam Nguyen and Dion Loy and Sam S. Gross and Cory Y. McLean and Mark A. DePristo},
	Doi = {10.1101/092890},
	Month = {dec},
	Publisher = {Cold Spring Harbor Laboratory},
	Title = {Creating a universal {SNP} and small indel variant caller with deep neural networks},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1101/092890}}

@article{Silver2017a,
  doi = {10.1038/nature24270},
  url = {https://doi.org/10.1038/nature24270},
  year  = {2017},
  month = {oct},
  publisher = {Springer Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  author = {David Silver and Julian Schrittwieser and Karen Simonyan and Ioannis Antonoglou and Aja Huang and Arthur Guez and Thomas Hubert and Lucas Baker and Matthew Lai and Adrian Bolton and Yutian Chen and Timothy Lillicrap and Fan Hui and Laurent Sifre and George van den Driessche and Thore Graepel and Demis Hassabis},
  title = {Mastering the game of Go without human knowledge},
  journal = {Nature}
}
@Article{Kratzwald2017,
  author      = {Bernhard Kratzwald and Zhiwu Huang and Danda Pani Paudel and Luc Van Gool},
  title       = {Towards an Understanding of Our World by GANing Videos in the Wild},
  abstract    = {Existing generative video models work well only for videos with a static background. For dynamic scenes, applications of these models demand an extra pre-processing step of background stabilization. In fact, the task of background stabilization may very often prove impossible for videos in the wild. To the best of our knowledge, we present the first video generation framework that works in the wild, without making any assumption on the videos' content. This allows us to avoid the background stabilization step, completely. The proposed method also outperforms the state-of-the-art methods even when the static background assumption is valid. This is achieved by designing a robust one-stream video generation architecture by exploiting Wasserstein GAN frameworks for better convergence. Since the proposed architecture is one-stream, which does not formally distinguish between fore- and background, it can generate - and learn from - videos with dynamic backgrounds. The superiority of our model is demonstrated by successfully applying it to three challenging problems: video colorization, video inpainting, and future prediction.},
  date        = {2017-11-30},
  eprint      = {1711.11453v1},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1711.11453v1:PDF},
  keywords    = {cs.CV},
}

@Article{Mehta2014,
  author      = {Pankaj Mehta and David J. Schwab},
  title       = {An exact mapping between the Variational Renormalization Group and Deep Learning},
  abstract    = {Deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data. Recently, such techniques have yielded record-breaking results on a diverse set of difficult machine learning tasks in computer vision, speech recognition, and natural language processing. Despite the enormous success of deep learning, relatively little is understood theoretically about why these techniques are so successful at feature learning and compression. Here, we show that deep learning is intimately related to one of the most important and successful techniques in theoretical physics, the renormalization group (RG). RG is an iterative coarse-graining scheme that allows for the extraction of relevant features (i.e. operators) as a physical system is examined at different length scales. We construct an exact mapping from the variational renormalization group, first introduced by Kadanoff, and deep learning architectures based on Restricted Boltzmann Machines (RBMs). We illustrate these ideas using the nearest-neighbor Ising Model in one and two-dimensions. Our results suggests that deep learning algorithms may be employing a generalized RG-like scheme to learn relevant features from data.},
  date        = {2014-10-14},
  eprint      = {1410.3831v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1410.3831v1:PDF},
  keywords    = {stat.ML, cond-mat.stat-mech, cs.LG, cs.NE},
}

@Article{Imambekov2012,
  author    = {Adilet Imambekov and Thomas L. Schmidt and Leonid I. Glazman},
  title     = {One-dimensional quantum liquids: Beyond the Luttinger liquid paradigm},
  journal   = {Reviews of Modern Physics},
  year      = {2012},
  volume    = {84},
  number    = {3},
  pages     = {1253--1306},
  month     = {sep},
  doi       = {10.1103/revmodphys.84.1253},
  publisher = {American Physical Society ({APS})},
}

@Article{Nieuwenburg2017,
  author      = {Evert van Nieuwenburg and Eyal Bairey and Gil Refael},
  title       = {Learning phase transitions from dynamics},
  abstract    = {We propose the use of recurrent neural networks for classifying phases of matter based on the dynamics of experimentally accessible observables. We demonstrate this approach by training recurrent networks on the magnetization traces of two distinct models of one-dimensional disordered and interacting spin chains. The obtained phase diagram for a well-studied model of the many-body localization transition shows excellent agreement with previously known results obtained from time-independent entanglement spectra. For a periodically-driven model featuring an inherently dynamical time-crystalline phase, the phase diagram that our network traces in a previously-unexplored regime coincides with an order parameter for its expected phases.},
  date        = {2017-12-01},
  eprint      = {1712.00450v1},
  eprintclass = {cond-mat.dis-nn},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1712.00450v1:PDF},
  keywords    = {cond-mat.dis-nn, cond-mat.str-el, quant-ph},
}

@Comment{jabref-meta: databaseType:bibtex;}


@article{slavnov90_noneq_time_curren_correl_funct,
  author =       {N. A. Slavnov},
  title =        {Nonequal-Time Current Correlation Function in a
                  One-Dimensional Bose Gas},
  journal =      {Theoretical and Mathematical Physics},
  volume =       82,
  number =       3,
  pages =        {273-282},
  year =         1990,
  doi =          {10.1007/bf01029221},
  url =          {https://doi.org/10.1007/bf01029221},
  DATE_ADDED =   {Mon Dec 18 13:32:44 2017},
}


@article{slavnov89_calcul_scalar_produc_wave_funct,
  author =       {N. A. Slavnov},
  title =        {Calculation of Scalar Products of Wave Functions and
                  Form Factors in the Framework of the Alcebraic Bethe
                  Ansatz},
  journal =      {Theoretical and Mathematical Physics},
  volume =       79,
  number =       2,
  pages =        {502-508},
  year =         1989,
  doi =          {10.1007/bf01016531},
  url =          {https://doi.org/10.1007/bf01016531},
  DATE_ADDED =   {Mon Dec 18 13:33:42 2017},
}


@article{goodfellow14_gener_adver_networ,
  author =       {Goodfellow, Ian J. and Pouget-Abadie, Jean and
                  Mirza, Mehdi and Xu, Bing and Warde-Farley, David
                  and Ozair, Sherjil and Courville, Aaron and Bengio,
                  Yoshua},
  title =        {Generative Adversarial Networks},
  journal =      {CoRR},
  year =         2014,
  url =          {http://arxiv.org/abs/1406.2661v1},
  abstract =     {We propose a new framework for estimating generative
                  models via an adversarial process, in which we
                  simultaneously train two models: a generative model
                  G that captures the data distribution, and a
                  discriminative model D that estimates the
                  probability that a sample came from the training
                  data rather than G. The training procedure for G is
                  to maximize the probability of D making a
                  mistake. This framework corresponds to a minimax
                  two-player game. In the space of arbitrary functions
                  G and D, a unique solution exists, with G recovering
                  the training data distribution and D equal to 1/2
                  everywhere. In the case where G and D are defined by
                  multilayer perceptrons, the entire system can be
                  trained with backpropagation. There is no need for
                  any Markov chains or unrolled approximate inference
                  networks during either training or generation of
                  samples.  Experiments demonstrate the potential of
                  the framework through qualitative and quantitative
                  evaluation of the generated samples.},
  archivePrefix ={arXiv},
  eprint =       {1406.2661},
  primaryClass = {stat.ML},
}

@article{cichocki92_neural_networ_solvin_system_linear,
  author =       {A. Cichocki and R. Unbehauen},
  title =        {Neural Networks for Solving Systems of Linear
                  Equations and Related Problems},
  journal =      {IEEE Transactions on Circuits and Systems I:
                  Fundamental Theory and Applications},
  volume =       39,
  number =       2,
  pages =        {124-138},
  year =         1992,
  doi =          {10.1109/81.167018},
  url =          {https://doi.org/10.1109/81.167018},
  DATE_ADDED =   {Tue Dec 19 16:08:09 2017},
}


@article{wang93_recur_neural_networ_solvin_linear_matrix_equat,
  author =       {J. Wang},
  title =        {Recurrent Neural Networks for Solving Linear Matrix
                  Equations},
  journal =      {Computers \& Mathematics with Applications},
  volume =       26,
  number =       9,
  pages =        {23-34},
  year =         1993,
  doi =          {10.1016/0898-1221(93)90003-e},
  url =          {https://doi.org/10.1016/0898-1221(93)90003-e},
  DATE_ADDED =   {Tue Dec 19 16:10:31 2017},
}


@article{cai17_approx_quant_many_body_wave,
  author =       {Cai, Zi and Liu, Jinguo},
  title =        {Approximating Quantum Many-Body Wave-Functions Using
                  Artificial Neural Networks},
  journal =      {CoRR},
  year =         2017,
  url =          {http://arxiv.org/abs/1704.05148v4},
  abstract =     {In this paper, we demonstrate the expressibility of
                  artificial neural networks (ANNs) in quantum
                  many-body physics by showing that a feed-forward
                  neural network with a small number of hidden layers
                  can be trained to approximate with high precision
                  the ground states of some notable quantum many-body
                  systems. We consider the one-dimensional free bosons
                  and fermions, spinless fermions on a square lattice
                  away from half-filling, as well as frustrated
                  quantum magnetism with a rapidly oscillating
                  ground-state characteristic function. In the latter
                  case, an ANN with a standard architecture fails,
                  while that with a slightly modified one successfully
                  learns the frustration-induced complex sign rule in
                  the ground state and approximates the ground states
                  with high precisions. As an example of practical use
                  of our method, we also perform the variational
                  method to explore the ground state of an
                  anti-ferromagnetic $J_1-J_2$ Heisenberg model.},
  archivePrefix ={arXiv},
  eprint =       {1704.05148},
  primaryClass = {cond-mat.str-el},
}

@article{dunjko17_machin_learn_artif_intel_quant_domain,
  author =       {Dunjko, Vedran and Briegel, Hans J.},
  title =        {Machine Learning \& Artificial Intelligence in the
                  Quantum Domain},
  journal =      {CoRR},
  year =         2017,
  url =          {http://arxiv.org/abs/1709.02779v1},
  abstract =     {Quantum information technologies, and intelligent
                  learning systems, are both emergent technologies
                  that will likely have a transforming impact on our
                  society. The respective underlying fields of
                  research -- quantum information (QI) versus machine
                  learning (ML) and artificial intelligence (AI) --
                  have their own specific challenges, which have
                  hitherto been investigated largely
                  independently. However, in a growing body of recent
                  work, researchers have been probing the question to
                  what extent these fields can learn and benefit from
                  each other. QML explores the interaction between
                  quantum computing and ML, investigating how results
                  and techniques from one field can be used to solve
                  the problems of the other. Recently, we have
                  witnessed breakthroughs in both directions of
                  influence. For instance, quantum computing is
                  finding a vital application in providing speed-ups
                  in ML, critical in our "big data" world.
                  Conversely, ML already permeates cutting-edge
                  technologies, and may become instrumental in
                  advanced quantum technologies. Aside from quantum
                  speed-up in data analysis, or classical ML
                  optimization used in quantum experiments, quantum
                  enhancements have also been demonstrated for
                  interactive learning, highlighting the potential of
                  quantum-enhanced learning agents. Finally, works
                  exploring the use of AI for the very design of
                  quantum experiments, and for performing parts of
                  genuine research autonomously, have reported their
                  first successes. Beyond the topics of mutual
                  enhancement, researchers have also broached the
                  fundamental issue of quantum generalizations of
                  ML/AI concepts.  This deals with questions of the
                  very meaning of learning and intelligence in a world
                  that is described by quantum mechanics. In this
                  review, we describe the main ideas, recent
                  developments, and progress in a broad spectrum of
                  research investigating machine learning and
                  artificial intelligence in the quantum domain.},
  archivePrefix ={arXiv},
  eprint =       {1709.02779},
  primaryClass = {quant-ph},
}

@article{poplin17_predic_cardiov_risk_factor_from,
  author =       {Poplin, Ryan and Varadarajan, Avinash V. and Blumer,
                  Katy and Liu, Yun and McConnell, Michael V. and
                  Corrado, Greg S. and Peng, Lily and Webster, Dale
                  R.},
  title =        {Predicting Cardiovascular Risk Factors From Retinal
                  Fundus Photographs Using Deep Learning},
  journal =      {CoRR},
  year =         2017,
  url =          {http://arxiv.org/abs/1708.09843v2},
  abstract =     {Traditionally, medical discoveries are made by
                  observing associations and then designing
                  experiments to test these hypotheses. However,
                  observing and quantifying associations in images can
                  be difficult because of the wide variety of
                  features, patterns, colors, values, shapes in real
                  data. In this paper, we use deep learning, a machine
                  learning technique that learns its own features, to
                  discover new knowledge from retinal fundus
                  images. Using models trained on data from 284,335
                  patients, and validated on two independent datasets
                  of 12,026 and 999 patients, we predict
                  cardiovascular risk factors not previously thought
                  to be present or quantifiable in retinal images,
                  such as such as age (within 3.26 years), gender
                  (0.97 AUC), smoking status (0.71 AUC), HbA1c (within
                  1.39 \%), systolic blood pressure (within 11.23mmHg)
                  as well as major adverse cardiac events (0.70
                  AUC). We further show that our models used distinct
                  aspects of the anatomy to generate each prediction,
                  such as the optic disc or blood vessels, opening
                  avenues of further research.},
  archivePrefix ={arXiv},
  eprint =       {1708.09843},
  primaryClass = {cs.CV},
}

