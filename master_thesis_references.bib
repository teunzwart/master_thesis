% Encoding: UTF-8

@article{Caux2009,
	Abstract = {Recent developments in the theory of integrable models have provided the means of calculating dynamical correlation functions of some important observables in systems such as Heisenberg spin chains and one-dimensional atomic gases. This article explicitly describes how such calculations are generally implemented in the ABACUS C++ library, emphasizing the universality in treatment of different cases coming as a consequence of unifying features within the Bethe Ansatz.},
	Author = {J.-S. Caux},
	Date = {2009-08-12},
	Doi = {10.1063/1.3216474},
	Eprint = {0908.1660v1},
	Eprintclass = {cond-mat.str-el},
	Eprinttype = {arXiv},
	File = {online:http\://arxiv.org/pdf/0908.1660v1:PDF},
	Journal = {J. Math. Phys.},
	Journaltitle = {J. Math. Phys. 50, 095214 (2009)},
	Keywords = {cond-mat.str-el, cond-mat.quant-gas},
	Owner = {teun},
	Pages = {095214},
	Title = {{Correlation functions of integrable models: a description of the ABACUS algorithm}},
	Volume = {50},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1063/1.3216474}}

@article{Saito2017,
	Abstract = {Motivated by the recent successful application of artificial neural networks to quantum many-body problems [G. Carleo and M. Troyer, Science {\bf 355}, 602 (2017)], a method to calculate the ground state of the Bose-Hubbard model using a feedforward neural network is proposed. The results are in good agreement with those obtained by exact diagonalization and the Gutzwiller approximation. The method of neural-network quantum states is promising for solving quantum many-body problems of ultracold atoms in optical lattices.},
	Author = {Hiroki Saito},
	Doi = {10.7566/jpsj.86.093001},
	Journal = {J. Phys. Soc. Jpn.},
	Pages = {093001},
	Title = {Solving the Bose-Hubbard model with machine learning},
	Volume = {86},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.7566/jpsj.86.093001}}

@article{Piroli2015,
	Author = {Lorenzo Piroli and Pasquale Calabrese},
	Doi = {10.1088/1751-8113/48/45/454002},
	Journal = {J. Phys. A: Math. Theor.},
	Pages = {454002},
	Title = {{Exact formulas for the form factors of local operators in the Lieb--Liniger model}},
	Volume = {48},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1751-8113/48/45/454002}}

@article{Caux2007,
	Author = {Jean-S{\'e}bastien Caux and Pasquale Calabrese and Nikita A Slavnov},
	Doi = {10.1088/1742-5468/2007/01/P01008},
	Journal = {J. Stat. Mech.},
	Pages = {P01008},
	Title = {{One-particle dynamical correlations in the one-dimensional Bose gas}},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-5468/2007/01/P01008}}

@article{Caux2007a,
	Author = {Jean-S{\'e}bastien Caux and Pasquale Calabrese},
	Doi = {10.1103/.74.031605},
	Journal = {Phys. Rev. A},
	Pages = {031605(R)},
	Title = {{Dynamical density-density correlations in the one-dimensional Bose gas}},
	Volume = {74},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevA.74.031605}}

@article{Griffiths1993,
	Author = {David J. Griffiths},
	Doi = {10.1088/0305-4470/26/9/021},
	Journal = {J. Phys. A: Math. Gen.},
	Pages = {2265},
	Title = {Boundary conditions at the derivative of a delta function},
	Volume = {26},
	Year = {1993},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/0305-4470/26/9/021}}

@article{Bethe1931,
	Author = {Hans Bethe},
	Doi = {10.1007/BF01341708},
	Journal = {Z. Phys.},
	Pages = {205},
	Title = {{Zur Theorie der Metalle I. Eigenwerte und Eigenfunktionen der linearen Atomkette}},
	Volume = {71},
	Year = {1931},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/BF01341708}}

@article{Lieb1963,
	Author = {Elliott H. Lieb and Werner Liniger},
	Doi = {10.1103/PhysRev.130.1605},
	Journal = {Phys. Rev.},
	Pages = {1605},
	Title = {{Exact Analysis of an Interacting Bose Gas. I. The General Solution and the Ground State}},
	Volume = {130},
	Year = {1963},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRev.130.1605}}

@article{Lieb1963a,
	Author = {Elliott H. Lieb},
	Doi = {10.1103/PhysRev.130.1616},
	Journal = {Phys. Rev.},
	Pages = {1616},
	Title = {{Exact Analysis of an Interacting Bose Gas. II. The Excitation Spectrum}},
	Volume = {130},
	Year = {1963},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRev.130.1616}}

@article{Carleo2017,
	Author = {Giuseppe Carleo and Matthias Troyer},
	Doi = {10.1126/science.aag2302},
	Journal = {Science},
	Pages = {602},
	Title = {Solving the quantum many-body problem with artificial neural networks},
	Volume = {355},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1126/science.aag2302}}

@book{Sutherland2004,
	Author = {Bill Sutherland},
	Isbn = {978-981-238-859-9},
	Publisher = {World Scientific},
	Title = {Beautiful models: 70 years of exactly solved quantum many-body problems},
	Year = {2004}}

@book{Franchini2017,
	Author = {Fabio Franchini},
	Doi = {10.1007/978-3-319-48487-7},
	Publisher = {Springer},
	Series = {Lecture Notes in Physics},
	Title = {An Introduction to Integrable Techniques for One-Dimensional Quantum Systems},
	Volume = {940},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-3-319-48487-7}}

@article{Cazalilla2011,
	Author = {M. A. Cazalilla and R. Citro and T. Giamarchi and E. Orignac and M. Rigol},
	Doi = {10.1103/RevModPhys.83.1405},
	Journal = {Rev. Mod. Phys.},
	Pages = {1405},
	Title = {One dimensional bosons: From condensed matter systems to ultracold gases},
	Volume = {83},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/RevModPhys.83.1405}}

@unpublished{Caux2015,
	Author = {Jean-S{\'e}bastien Caux},
	Month = jun,
	Note = {Lectures for S{\'e}minaire d'excellence en physique, Paris, June 2015},
	Title = {Integrability in atomic and condensed matter physics in and out of equilibrium},
	Year = {2015}}

@article{Yang1969,
	Author = {C. N. Yang and C. P. Yang},
	Doi = {10.1063/1.1664947},
	Journal = {Journal of Mathematical Physics},
	Pages = {1115},
	Title = {Thermodynamics of a One-Dimensional System of Bosons with Repulsive Delta-Function Interaction},
	Volume = {10},
	Year = {1969},
	Bdsk-Url-1 = {http://dx.doi.org/10.1063/1.1664947}}

@article{Panfil2014,
	Author = {Mi{\l}osz Panfil and Jean-S{\'e}bastien Caux},
	Doi = {10.1103/PhysRevA.89.033605},
	Journal = {Physical Review A},
	Pages = {033605},
	Title = {Finite-temperature correlations in the Lieb-Liniger one-dimensional Bose gas},
	Volume = {89},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevA.89.033605}}

@article{Gu2005,
	Author = {S.-J. Gu and N.M.R. Peres and Y.-Q. Li},
	Doi = {10.1140/epjb/e2005-00390-1},
	Journal = {Eur. Phys. J. B},
	Pages = {157},
	Title = {Numerical and Monte Carlo Bethe ansatz method: 1D Heisenberg model},
	Volume = {48},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1140/epjb/e2005-00390-1}}

@article{Nardis2015,
	Author = {J. De Nardis and M. Panfil},
	Doi = {10.1088/1742-5468/2015/02/P02019},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Number = {P02019},
	Title = {Density form factors of the 1D Bose gas for finite entropy states},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-5468/2015/02/P02019}}

@article{Nardis2016,
	Author = {Jacopo De Nardis and Mi{\l}osz Panfil},
	Doi = {10.21468/SciPostPhys.1.2.015},
	Journal = {SciPost Phys.},
	Number = {015},
	Title = {Exact correlations in the Lieb-Liniger model and detailed balance out-of-equilibrium},
	Volume = {1},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.21468/SciPostPhys.1.2.015}}

@book{Russell2010,
	Author = {Stuart Russell and Peter Norvig},
	Edition = {3rd edition},
	Publisher = {Pearson},
	Title = {Artificial Intelligence: A Modern Approach},
	Year = {2010}}

@book{Press2007,
	Author = {William H. Press and Saul A. Teukolsky and William T. Vetterling and Brian P. Flannery},
	Publisher = {Cambridge University Press},
	Title = {Numerical Recipes: The Art of Scientific Computing},
	Year = {2007}}

@article{Fabbri2015,
	Author = {N. Fabbri and M. Panfil and D. Cl{\'e}ment and L. Fallani and M. Inguscio and C. Fort and J.-S. Caux},
	Doi = {10.1103/PhysRevA.91.043617},
	Journal = {Physical Review A},
	Number = {043617},
	Title = {Dynamical structure factor of one-dimensional Bose gases: Experimental signatures of beyond-Luttinger-liquid physics},
	Volume = {91},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevA.91.043617}}

@article{Tongeren2016,
	Author = {Stijn J. van Tongeren},
	Keywords = {arXiv:1606.02951v3},
	Title = {Introduction to the thermodynamic Bethe ansatz},
	Year = {2016}}

@Unpublished{Liu2017,
  author = {Zhaocheng Liu and Sean P. Rodrigues and Wenshan Cai},
  title  = {Simulating the Ising Model with a Deep Convolutional Generative Adversarial Network},
  note   = {arXiv:1710.04987},
}

@article{Lang2017,
	Author = {Guillaume Lang and Frank Hekking and and Anna Minguzzi},
	Doi = {10.21468/SciPostPhys.3.1.003},
	Journal = {SciPost Phys.},
	Number = {001},
	Title = {Ground-state energy and excitation spectrum of the Lieb-Liniger model: accurate analytical results and conjectures about the exact solution},
	Volume = {3},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.21468/SciPostPhys.3.1.003}}

@book{Zemyan2012,
	Author = {S. M. Zemyan},
	Doi = {10.1007/978-0-8176-8349-8_2},
	Publisher = {Springer Science},
	Title = {The Classical Theory of Integral Equations: A Concise Treatment},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-0-8176-8349-8_2}}

@misc{tofind,
	Title = {{\color{red} find references}}}

@book{Sueli2003,
	Author = {Endre S{\"u}li and David Mayers},
	Isbn = {9780521007948},
	Publisher = {Cambridge University Press},
	Title = {An introduction to numerical analysis},
	Year = {2003}}

@misc{tensorflow2015-whitepaper,
	Author = {Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dan~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
	Note = {Software available from tensorflow.org},
	Title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	Url = {https://www.tensorflow.org/},
	Year = {2015},
	Bdsk-Url-1 = {https://www.tensorflow.org/}}

@article{Radford2015,
	Abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	Author = {Alec Radford and Luke Metz and Soumith Chintala},
	Date = {2015-11-19},
	Eprint = {1511.06434v2},
	Eprintclass = {cs.LG},
	Eprinttype = {arXiv},
	File = {online:http\://arxiv.org/pdf/1511.06434v2:PDF},
	Keywords = {cs.LG, cs.CV},
	Title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}}

@article{Carrasquilla2017,
	Author = {Juan Carrasquilla and Roger G. Melko},
	Doi = {10.1038/nphys4035},
	Journal = {Nature Physics},
	Month = {feb},
	Number = {5},
	Pages = {431--434},
	Publisher = {Springer Nature},
	Title = {Machine learning phases of matter},
	Volume = {13},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1038/nphys4035}}

@article{Xia2007,
	Author = {Youshen Xia and Gang Feng},
	Doi = {10.1016/j.neunet.2007.01.001},
	Journal = {Neural Networks},
	Month = {jul},
	Number = {5},
	Pages = {577--589},
	Publisher = {Elsevier {BV}},
	Title = {A new neural network for solving nonlinear projection equations},
	Volume = {20},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.neunet.2007.01.001}}

@article{Mathia1995,
	Author = {Karl Mathia and Richard Saeks},
	Journal = {World Congress on Neural Networks},
	Title = {Solving Nonlinear Equations Using Recurrent Neural Networks},
	Year = {1995}}

@article{Caux2016,
	Author = {Jean-S{\'{e}}bastien Caux},
	Doi = {10.1088/1742-5468/2016/06/064006},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Month = {jun},
	Number = {6},
	Pages = {064006},
	Publisher = {{IOP} Publishing},
	Title = {The Quench Action},
	Volume = {2016},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1088/1742-5468/2016/06/064006}}

@standard{cppstandard2016,
	Institution = {International Organization for Standardization},
	Organization = {INCITS/ISO/IEC 14882:2014},
	Title = {Standard for Programming Language C++},
	Year = {2016}}

@misc{ieeefp2008,
	Doi = {10.1109/ieeestd.2008.4610935},
	Publisher = {{IEEE}},
	Title = {754-2008 - {IEEE} Standard for Floating-Point Arithmetic},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ieeestd.2008.4610935}}

@article{Nik2009,
	Author = {N. M. A. Nik and Zainidin Eshkuvatov and Mohammad Yaghobifar and M. Hasan},
	Booktitle = {International Journal of Computational and Mathematical Sciences},
	Month = {01},
	Title = {Numerical Solution of Infinite Boundary Integral Equation by using Galerkin Method with Laguerre Polynomials},
	Volume = {3},
	Year = {2009}}

@article{Kivinen2004,
	Author = {J. Kivinen and A.J. Smola and R.C. Williamson},
	Doi = {10.1109/tsp.2004.830991},
	Journal = {{IEEE} Transactions on Signal Processing},
	Month = {aug},
	Number = {8},
	Pages = {2165--2176},
	Publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	Title = {Online Learning with Kernels},
	Volume = {52},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/tsp.2004.830991}}

@article{Dunjko2017,
	Abstract = {Quantum information technologies, and intelligent learning systems, are both emergent technologies that will likely have a transforming impact on our society. The respective underlying fields of research -- quantum information (QI) versus machine learning (ML) and artificial intelligence (AI) -- have their own specific challenges, which have hitherto been investigated largely independently. However, in a growing body of recent work, researchers have been probing the question to what extent these fields can learn and benefit from each other. QML explores the interaction between quantum computing and ML, investigating how results and techniques from one field can be used to solve the problems of the other. Recently, we have witnessed breakthroughs in both directions of influence. For instance, quantum computing is finding a vital application in providing speed-ups in ML, critical in our "big data" world. Conversely, ML already permeates cutting-edge technologies, and may become instrumental in advanced quantum technologies. Aside from quantum speed-up in data analysis, or classical ML optimization used in quantum experiments, quantum enhancements have also been demonstrated for interactive learning, highlighting the potential of quantum-enhanced learning agents. Finally, works exploring the use of AI for the very design of quantum experiments, and for performing parts of genuine research autonomously, have reported their first successes. Beyond the topics of mutual enhancement, researchers have also broached the fundamental issue of quantum generalizations of ML/AI concepts. This deals with questions of the very meaning of learning and intelligence in a world that is described by quantum mechanics. In this review, we describe the main ideas, recent developments, and progress in a broad spectrum of research investigating machine learning and artificial intelligence in the quantum domain.},
	Author = {Vedran Dunjko and Hans J. Briegel},
	Date = {2017-09-08},
	Eprint = {1709.02779v1},
	Eprintclass = {quant-ph},
	Eprinttype = {arXiv},
	File = {online:http\://arxiv.org/pdf/1709.02779v1:PDF},
	Keywords = {quant-ph, cs.AI, cs.CV},
	Title = {Machine learning \& artificial intelligence in the quantum domain}}

@Book{Korepin1993,
  author =    {V. E. Korepin and N. M. Bogoliobov and A. G. Izergin},
  title =        {Quantum inverse scattering method and correlation functions},
  publisher =    {Cambridge University Press},
  year =         {1993},
}

@article{Cai2017,
	Abstract = {In this paper, we demonstrate the expressibility of artificial neural networks (ANNs) in quantum many-body physics by showing that a feed-forward neural network with a small number of hidden layers can be trained to approximate with high precision the ground states of some notable quantum many-body systems. We consider the one-dimensional free bosons and fermions, spinless fermions on a square lattice away from half-filling, as well as frustrated quantum magnetism with a rapidly oscillating ground-state characteristic function. In the latter case, an ANN with a standard architecture fails, while that with a slightly modified one successfully learns the frustration-driven complex sign rule in the ground state. The practical application of this method to explore the unknown ground states is also discussed.},
	Author = {Zi Cai},
	Date = {2017-04-17},
	Eprint = {1704.05148v3},
	Eprintclass = {cond-mat.str-el},
	Eprinttype = {arXiv},
	File = {online:http\://arxiv.org/pdf/1704.05148v3:PDF},
	Keywords = {cond-mat.str-el},
	Title = {Approximating quantum many-body wave-functions using artificial neural networks}}

@article{Silver2017,
	Abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
	Author = {David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
	Date = {2017-12-05},
	Eprint = {1712.01815v1},
	Eprintclass = {cs.AI},
	Eprinttype = {arXiv},
	File = {online:http\://arxiv.org/pdf/1712.01815v1:PDF},
	Keywords = {cs.AI, cs.LG},
	Title = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}}

@article{Poplin2016,
	Author = {Ryan Poplin and Dan Newburger and Jojo Dijamco and Nam Nguyen and Dion Loy and Sam S. Gross and Cory Y. McLean and Mark A. DePristo},
	Doi = {10.1101/092890},
	Month = {dec},
	Publisher = {Cold Spring Harbor Laboratory},
	Title = {Creating a universal {SNP} and small indel variant caller with deep neural networks},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1101/092890}}

@article{Silver2017a,
  doi = {10.1038/nature24270},
  url = {https://doi.org/10.1038/nature24270},
  year  = {2017},
  month = {oct},
  publisher = {Springer Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  author = {David Silver and Julian Schrittwieser and Karen Simonyan and Ioannis Antonoglou and Aja Huang and Arthur Guez and Thomas Hubert and Lucas Baker and Matthew Lai and Adrian Bolton and Yutian Chen and Timothy Lillicrap and Fan Hui and Laurent Sifre and George van den Driessche and Thore Graepel and Demis Hassabis},
  title = {Mastering the game of Go without human knowledge},
  journal = {Nature}
}
@Article{Kratzwald2017,
  author      = {Bernhard Kratzwald and Zhiwu Huang and Danda Pani Paudel and Luc Van Gool},
  title       = {Towards an Understanding of Our World by GANing Videos in the Wild},
  abstract    = {Existing generative video models work well only for videos with a static background. For dynamic scenes, applications of these models demand an extra pre-processing step of background stabilization. In fact, the task of background stabilization may very often prove impossible for videos in the wild. To the best of our knowledge, we present the first video generation framework that works in the wild, without making any assumption on the videos' content. This allows us to avoid the background stabilization step, completely. The proposed method also outperforms the state-of-the-art methods even when the static background assumption is valid. This is achieved by designing a robust one-stream video generation architecture by exploiting Wasserstein GAN frameworks for better convergence. Since the proposed architecture is one-stream, which does not formally distinguish between fore- and background, it can generate - and learn from - videos with dynamic backgrounds. The superiority of our model is demonstrated by successfully applying it to three challenging problems: video colorization, video inpainting, and future prediction.},
  date        = {2017-11-30},
  eprint      = {1711.11453v1},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1711.11453v1:PDF},
  keywords    = {cs.CV},
}

@Article{Mehta2014,
  author      = {Pankaj Mehta and David J. Schwab},
  title       = {An exact mapping between the Variational Renormalization Group and Deep Learning},
  abstract    = {Deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data. Recently, such techniques have yielded record-breaking results on a diverse set of difficult machine learning tasks in computer vision, speech recognition, and natural language processing. Despite the enormous success of deep learning, relatively little is understood theoretically about why these techniques are so successful at feature learning and compression. Here, we show that deep learning is intimately related to one of the most important and successful techniques in theoretical physics, the renormalization group (RG). RG is an iterative coarse-graining scheme that allows for the extraction of relevant features (i.e. operators) as a physical system is examined at different length scales. We construct an exact mapping from the variational renormalization group, first introduced by Kadanoff, and deep learning architectures based on Restricted Boltzmann Machines (RBMs). We illustrate these ideas using the nearest-neighbor Ising Model in one and two-dimensions. Our results suggests that deep learning algorithms may be employing a generalized RG-like scheme to learn relevant features from data.},
  date        = {2014-10-14},
  eprint      = {1410.3831v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1410.3831v1:PDF},
  keywords    = {stat.ML, cond-mat.stat-mech, cs.LG, cs.NE},
}

@Article{Imambekov2012,
  author    = {Adilet Imambekov and Thomas L. Schmidt and Leonid I. Glazman},
  title     = {One-dimensional quantum liquids: Beyond the Luttinger liquid paradigm},
  journal   = {Reviews of Modern Physics},
  year      = {2012},
  volume    = {84},
  number    = {3},
  pages     = {1253--1306},
  month     = {sep},
  doi       = {10.1103/revmodphys.84.1253},
  publisher = {American Physical Society ({APS})},
}

@Article{Nieuwenburg2017,
  author      = {Evert van Nieuwenburg and Eyal Bairey and Gil Refael},
  title       = {Learning phase transitions from dynamics},
  abstract    = {We propose the use of recurrent neural networks for classifying phases of matter based on the dynamics of experimentally accessible observables. We demonstrate this approach by training recurrent networks on the magnetization traces of two distinct models of one-dimensional disordered and interacting spin chains. The obtained phase diagram for a well-studied model of the many-body localization transition shows excellent agreement with previously known results obtained from time-independent entanglement spectra. For a periodically-driven model featuring an inherently dynamical time-crystalline phase, the phase diagram that our network traces in a previously-unexplored regime coincides with an order parameter for its expected phases.},
  date        = {2017-12-01},
  eprint      = {1712.00450v1},
  eprintclass = {cond-mat.dis-nn},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1712.00450v1:PDF},
  keywords    = {cond-mat.dis-nn, cond-mat.str-el, quant-ph},
}

@Comment{jabref-meta: databaseType:bibtex;}


@article{slavnov90_noneq_time_curren_correl_funct,
  author =       {N. A. Slavnov},
  title =        {Nonequal-Time Current Correlation Function in a
                  One-Dimensional Bose Gas},
  journal =      {Theoretical and Mathematical Physics},
  volume =       82,
  number =       3,
  pages =        {273-282},
  year =         1990,
  doi =          {10.1007/bf01029221},
  url =          {https://doi.org/10.1007/bf01029221},
  DATE_ADDED =   {Mon Dec 18 13:32:44 2017},
}


@article{slavnov89_calcul_scalar_produc_wave_funct,
  author =       {N. A. Slavnov},
  title =        {Calculation of Scalar Products of Wave Functions and
                  Form Factors in the Framework of the Alcebraic Bethe
                  Ansatz},
  journal =      {Theoretical and Mathematical Physics},
  volume =       79,
  number =       2,
  pages =        {502-508},
  year =         1989,
  doi =          {10.1007/bf01016531},
  url =          {https://doi.org/10.1007/bf01016531},
  DATE_ADDED =   {Mon Dec 18 13:33:42 2017},
}


@article{goodfellow14_gener_adver_networ,
  author =       {Goodfellow, Ian J. and Pouget-Abadie, Jean and
                  Mirza, Mehdi and Xu, Bing and Warde-Farley, David
                  and Ozair, Sherjil and Courville, Aaron and Bengio,
                  Yoshua},
  title =        {Generative Adversarial Networks},
  journal =      {CoRR},
  year =         2014,
  url =          {http://arxiv.org/abs/1406.2661v1},
  abstract =     {We propose a new framework for estimating generative
                  models via an adversarial process, in which we
                  simultaneously train two models: a generative model
                  G that captures the data distribution, and a
                  discriminative model D that estimates the
                  probability that a sample came from the training
                  data rather than G. The training procedure for G is
                  to maximize the probability of D making a
                  mistake. This framework corresponds to a minimax
                  two-player game. In the space of arbitrary functions
                  G and D, a unique solution exists, with G recovering
                  the training data distribution and D equal to 1/2
                  everywhere. In the case where G and D are defined by
                  multilayer perceptrons, the entire system can be
                  trained with backpropagation. There is no need for
                  any Markov chains or unrolled approximate inference
                  networks during either training or generation of
                  samples.  Experiments demonstrate the potential of
                  the framework through qualitative and quantitative
                  evaluation of the generated samples.},
  archivePrefix ={arXiv},
  eprint =       {1406.2661},
  primaryClass = {stat.ML},
}

@article{cichocki92_neural_networ_solvin_system_linear,
  author =       {A. Cichocki and R. Unbehauen},
  title =        {Neural Networks for Solving Systems of Linear
                  Equations and Related Problems},
  journal =      {IEEE Transactions on Circuits and Systems I:
                  Fundamental Theory and Applications},
  volume =       39,
  number =       2,
  pages =        {124-138},
  year =         1992,
  doi =          {10.1109/81.167018},
  url =          {https://doi.org/10.1109/81.167018},
  DATE_ADDED =   {Tue Dec 19 16:08:09 2017},
}


@article{wang93_recur_neural_networ_solvin_linear_matrix_equat,
  author =       {J. Wang},
  title =        {Recurrent Neural Networks for Solving Linear Matrix
                  Equations},
  journal =      {Computers \& Mathematics with Applications},
  volume =       26,
  number =       9,
  pages =        {23-34},
  year =         1993,
  doi =          {10.1016/0898-1221(93)90003-e},
  url =          {https://doi.org/10.1016/0898-1221(93)90003-e},
  DATE_ADDED =   {Tue Dec 19 16:10:31 2017},
}


@article{cai17_approx_quant_many_body_wave,
  author =       {Cai, Zi and Liu, Jinguo},
  title =        {Approximating Quantum Many-Body Wave-Functions Using
                  Artificial Neural Networks},
  journal =      {CoRR},
  year =         2017,
  url =          {http://arxiv.org/abs/1704.05148v4},
  abstract =     {In this paper, we demonstrate the expressibility of
                  artificial neural networks (ANNs) in quantum
                  many-body physics by showing that a feed-forward
                  neural network with a small number of hidden layers
                  can be trained to approximate with high precision
                  the ground states of some notable quantum many-body
                  systems. We consider the one-dimensional free bosons
                  and fermions, spinless fermions on a square lattice
                  away from half-filling, as well as frustrated
                  quantum magnetism with a rapidly oscillating
                  ground-state characteristic function. In the latter
                  case, an ANN with a standard architecture fails,
                  while that with a slightly modified one successfully
                  learns the frustration-induced complex sign rule in
                  the ground state and approximates the ground states
                  with high precisions. As an example of practical use
                  of our method, we also perform the variational
                  method to explore the ground state of an
                  anti-ferromagnetic $J_1-J_2$ Heisenberg model.},
  archivePrefix ={arXiv},
  eprint =       {1704.05148},
  primaryClass = {cond-mat.str-el},
}

@article{dunjko17_machin_learn_artif_intel_quant_domain,
  author =       {Dunjko, Vedran and Briegel, Hans J.},
  title =        {Machine Learning \& Artificial Intelligence in the
                  Quantum Domain},
  journal =      {CoRR},
  year =         2017,
  url =          {http://arxiv.org/abs/1709.02779v1},
  abstract =     {Quantum information technologies, and intelligent
                  learning systems, are both emergent technologies
                  that will likely have a transforming impact on our
                  society. The respective underlying fields of
                  research -- quantum information (QI) versus machine
                  learning (ML) and artificial intelligence (AI) --
                  have their own specific challenges, which have
                  hitherto been investigated largely
                  independently. However, in a growing body of recent
                  work, researchers have been probing the question to
                  what extent these fields can learn and benefit from
                  each other. QML explores the interaction between
                  quantum computing and ML, investigating how results
                  and techniques from one field can be used to solve
                  the problems of the other. Recently, we have
                  witnessed breakthroughs in both directions of
                  influence. For instance, quantum computing is
                  finding a vital application in providing speed-ups
                  in ML, critical in our "big data" world.
                  Conversely, ML already permeates cutting-edge
                  technologies, and may become instrumental in
                  advanced quantum technologies. Aside from quantum
                  speed-up in data analysis, or classical ML
                  optimization used in quantum experiments, quantum
                  enhancements have also been demonstrated for
                  interactive learning, highlighting the potential of
                  quantum-enhanced learning agents. Finally, works
                  exploring the use of AI for the very design of
                  quantum experiments, and for performing parts of
                  genuine research autonomously, have reported their
                  first successes. Beyond the topics of mutual
                  enhancement, researchers have also broached the
                  fundamental issue of quantum generalizations of
                  ML/AI concepts.  This deals with questions of the
                  very meaning of learning and intelligence in a world
                  that is described by quantum mechanics. In this
                  review, we describe the main ideas, recent
                  developments, and progress in a broad spectrum of
                  research investigating machine learning and
                  artificial intelligence in the quantum domain.},
  archivePrefix ={arXiv},
  eprint =       {1709.02779},
  primaryClass = {quant-ph},
}

@article{poplin17_predic_cardiov_risk_factor_from,
  author =       {Poplin, Ryan and Varadarajan, Avinash V. and Blumer,
                  Katy and Liu, Yun and McConnell, Michael V. and
                  Corrado, Greg S. and Peng, Lily and Webster, Dale
                  R.},
  title =        {Predicting Cardiovascular Risk Factors From Retinal
                  Fundus Photographs Using Deep Learning},
  journal =      {CoRR},
  year =         2017,
  url =          {http://arxiv.org/abs/1708.09843v2},
  abstract =     {Traditionally, medical discoveries are made by
                  observing associations and then designing
                  experiments to test these hypotheses. However,
                  observing and quantifying associations in images can
                  be difficult because of the wide variety of
                  features, patterns, colors, values, shapes in real
                  data. In this paper, we use deep learning, a machine
                  learning technique that learns its own features, to
                  discover new knowledge from retinal fundus
                  images. Using models trained on data from 284,335
                  patients, and validated on two independent datasets
                  of 12,026 and 999 patients, we predict
                  cardiovascular risk factors not previously thought
                  to be present or quantifiable in retinal images,
                  such as such as age (within 3.26 years), gender
                  (0.97 AUC), smoking status (0.71 AUC), HbA1c (within
                  1.39 \%), systolic blood pressure (within 11.23mmHg)
                  as well as major adverse cardiac events (0.70
                  AUC). We further show that our models used distinct
                  aspects of the anatomy to generate each prediction,
                  such as the optic disc or blood vessels, opening
                  avenues of further research.},
  archivePrefix ={arXiv},
  eprint =       {1708.09843},
  primaryClass = {cs.CV},
}

@article{paganini17_machin_learn_algor_jet_taggin,
  author =       {Paganini, Michela},
  title =        {Machine Learning Algorithms for $b$-Jet Tagging At
                  the Atlas Experiment},
  journal =      {CoRR},
  year =         2017,
  url =          {http://arxiv.org/abs/1711.08811v1},
  abstract =     {The separation of $b$-quark initiated jets from
                  those coming from lighter quark flavors
                  ($b$-tagging) is a fundamental tool for the ATLAS
                  physics program at the CERN Large Hadron
                  Collider. The most powerful $b$-tagging algorithms
                  combine information from low-level taggers,
                  exploiting reconstructed track and vertex
                  information, into machine learning classifiers. The
                  potential of modern deep learning techniques is
                  explored using simulated events, and compared to
                  that achievable from more traditional classifiers
                  such as boosted decision trees.},
  archivePrefix ={arXiv},
  eprint =       {1711.08811},
  primaryClass = {hep-ex},
}

@article{ball10_data_minin_and_machin_learn_in_astron,
  author =       {NICHOLAS M. BALL and ROBERT J. BRUNNER},
  title =        {Data Mining AND Machine Learning IN Astronomy},
  journal =      {International Journal of Modern Physics D},
  volume =       19,
  number =       07,
  pages =        {1049-1106},
  year =         2010,
  doi =          {10.1142/s0218271810017160},
  url =          {https://doi.org/10.1142/s0218271810017160},
  DATE_ADDED =   {Mon Jan 15 11:25:55 2018},
}


@article{mnih13_playin_atari_with_deep_reinf_learn,
  author =       {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver,
                  David and Graves, Alex and Antonoglou, Ioannis and
                  Wierstra, Daan and Riedmiller, Martin},
  title =        {Playing Atari With Deep Reinforcement Learning},
  journal =      {CoRR},
  year =         2013,
  url =          {http://arxiv.org/abs/1312.5602v1},
  abstract =     {We present the first deep learning model to
                  successfully learn control policies directly from
                  high-dimensional sensory input using reinforcement
                  learning. The model is a convolutional neural
                  network, trained with a variant of Q-learning, whose
                  input is raw pixels and whose output is a value
                  function estimating future rewards. We apply our
                  method to seven Atari 2600 games from the Arcade
                  Learning Environment, with no adjustment of the
                  architecture or learning algorithm. We find that it
                  outperforms all previous approaches on six of the
                  games and surpasses a human expert on three of
                  them.},
  archivePrefix ={arXiv},
  eprint =       {1312.5602v1},
  primaryClass = {cs.LG},
}

@InProceedings{pmlr-v37-clark15,
  title = 	 {Training Deep Convolutional Neural Networks to Play Go},
  author = 	 {Christopher Clark and Amos Storkey},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1766--1774},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/clark15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/clark15.html},
  abstract = 	 {Mastering the game of Go has remained a long-standing challenge to the field of AI. Modern computer Go programs rely on processing millions of possible future positions to play well, but intuitively a stronger and more ’humanlike’ way to play the game would be to rely on pattern recognition rather than brute force computation. Following this sentiment, we train deep convolutional neural networks to play Go by training them to predict the moves made by expert Go players. To solve this problem we introduce a number of novel techniques, including a method of tying weights in the network to ’hard code’ symmetries that are expected to exist in the target function, and demonstrate in an ablation study they considerably improve performance. Our final networks are able to achieve move prediction accuracies of 41.1% and 44.4% on two different Go datasets, surpassing previous state of the art on this task by significant margins. Additionally, while previous move prediction systems have not yielded strong Go playing programs, we show that the networks trained in this work acquired high levels of skill. Our convolutional neural networks can consistently defeat the well known Go program GNU Go and win some games against state of the art Go playing program Fuego while using a fraction of the play time.}
}


@Article{Cherny2008,
  author =       {Alexander Yu Cherny and Joachim Brand},
  title =        {Approximate expression for the dynamic structure factor in the Lieb-Liniger model},
  journal =      {J. Phys.: Conf. Ser.},
  year =         {2008},
  volume =    {129},
  number =    {012051},
}

@article{lillicrap15_contin_contr_with_deep_reinf_learn,
  author =       {Lillicrap, Timothy P. and Hunt, Jonathan J. and
                  Pritzel, Alexander and Heess, Nicolas and Erez, Tom
                  and Tassa, Yuval and Silver, David and Wierstra,
                  Daan},
  title =        {Continuous Control With Deep Reinforcement Learning},
  journal =      {CoRR},
  year =         2015,
  url =          {http://arxiv.org/abs/1509.02971v5},
  abstract =     {We adapt the ideas underlying the success of Deep
                  Q-Learning to the continuous action domain. We
                  present an actor-critic, model-free algorithm based
                  on the deterministic policy gradient that can
                  operate over continuous action spaces. Using the
                  same learning algorithm, network architecture and
                  hyper-parameters, our algorithm robustly solves more
                  than 20 simulated physics tasks, including classic
                  problems such as cartpole swing-up, dexterous
                  manipulation, legged locomotion and car driving. Our
                  algorithm is able to find policies whose performance
                  is competitive with those found by a planning
                  algorithm with full access to the dynamics of the
                  domain and its derivatives.  We further demonstrate
                  that for many of the tasks the algorithm can learn
                  policies end-to-end: directly from raw pixel
                  inputs.},
  archivePrefix ={arXiv},
  eprint =       {1509.02971},
  primaryClass = {cs.LG},
}

@article{hasselt15_deep_reinf_learn_with_doubl_q_learn,
  author =       {Hasselt, Hado van and Guez, Arthur and Silver,
                  David},
  title =        {Deep Reinforcement Learning With Double Q-Learning},
  journal =      {CoRR},
  year =         2015,
  url =          {http://arxiv.org/abs/1509.06461v3},
  abstract =     {The popular Q-learning algorithm is known to
                  overestimate action values under certain
                  conditions. It was not previously known whether, in
                  practice, such overestimations are common, whether
                  they harm performance, and whether they can
                  generally be prevented. In this paper, we answer all
                  these questions affirmatively. In particular, we
                  first show that the recent DQN algorithm, which
                  combines Q-learning with a deep neural network,
                  suffers from substantial overestimations in some
                  games in the Atari 2600 domain. We then show that
                  the idea behind the Double Q-learning algorithm,
                  which was introduced in a tabular setting, can be
                  generalized to work with large-scale function
                  approximation. We propose a specific adaptation to
                  the DQN algorithm and show that the resulting
                  algorithm not only reduces the observed
                  overestimations, as hypothesized, but that this also
                  leads to much better performance on several games.},
  archivePrefix ={arXiv},
  eprint =       {1509.06461},
  primaryClass = {cs.LG},
}

@article{hausknecht15_deep_recur_q_learn_partial_obser_mdps,
  author =       {Hausknecht, Matthew and Stone, Peter},
  title =        {Deep Recurrent Q-Learning for Partially Observable
                  Mdps},
  journal =      {CoRR},
  year =         2015,
  url =          {http://arxiv.org/abs/1507.06527v4},
  abstract =     {Deep Reinforcement Learning has yielded proficient
                  controllers for complex tasks. However, these
                  controllers have limited memory and rely on being
                  able to perceive the complete game screen at each
                  decision point. To address these shortcomings, this
                  article investigates the effects of adding
                  recurrency to a Deep Q-Network (DQN) by replacing
                  the first post-convolutional fully-connected layer
                  with a recurrent LSTM. The resulting \textit{Deep
                  Recurrent Q-Network} (DRQN), although capable of
                  seeing only a single frame at each timestep,
                  successfully integrates information through time and
                  replicates DQN's performance on standard Atari games
                  and partially observed equivalents featuring
                  flickering game screens. Additionally, when trained
                  with partial observations and evaluated with
                  incrementally more complete observations, DRQN's
                  performance scales as a function of
                  observability. Conversely, when trained with full
                  observations and evaluated with partial
                  observations, DRQN's performance degrades less than
                  DQN's. Thus, given the same length of history,
                  recurrency is a viable alternative to stacking a
                  history of frames in the DQN's input layer and while
                  recurrency confers no systematic advantage when
                  learning to play the game, the recurrent net can
                  better adapt at evaluation time if the quality of
                  observations changes.},
  archivePrefix ={arXiv},
  eprint =       {1507.06527},
  primaryClass = {cs.LG},
}

@Book{pitaevskii,
  author =    {Lev Pitaevski and Sandro Stringari},
  title =        {Bose-Einstein Condensation},
  publisher =   {Clarendon Press},
  year =         {2003},
  volume =    {116},
  series =    {Inernational Series of Monographs on Physics},
  address =   {Oxford},  
}

@article{silver16_master_game_go_with_deep,
  author =       {David Silver and Aja Huang and Chris J. Maddison and
                  Arthur Guez and Laurent Sifre and George van den
                  Driessche and Julian Schrittwieser and Ioannis
                  Antonoglou and Veda Panneershelvam and Marc Lanctot
                  and Sander Dieleman and Dominik Grewe and John Nham
                  and Nal Kalchbrenner and Ilya Sutskever and Timothy
                  Lillicrap and Madeleine Leach and Koray Kavukcuoglu
                  and Thore Graepel and Demis Hassabis},
  title =        {Mastering the Game of Go With Deep Neural Networks
                  and Tree Search},
  journal =      {Nature},
  volume =       529,
  number =       7587,
  pages =        {484-489},
  year =         2016,
  doi =          {10.1038/nature16961},
  url =          {https://doi.org/10.1038/nature16961},
  DATE_ADDED =   {Wed Feb 14 11:46:36 2018},
}


@article{watkins92_q_learn,
  author =       {Christopher J. C. H. Watkins and Peter Dayan},
  title =        {Q-Learning},
  journal =      {Machine Learning},
  volume =       8,
  number =       {3-4},
  pages =        {279-292},
  year =         1992,
  doi =          {10.1007/bf00992698},
  url =          {https://doi.org/10.1007/bf00992698},
  DATE_ADDED =   {Wed Feb 14 11:48:14 2018},
}


@article{mnih15_human_level_contr_throug_deep_reinf_learn,
  author =       {Volodymyr Mnih and Koray Kavukcuoglu and David
                  Silver and Andrei A. Rusu and Joel Veness and Marc
                  G. Bellemare and Alex Graves and Martin Riedmiller
                  and Andreas K. Fidjeland and Georg Ostrovski and
                  Stig Petersen and Charles Beattie and Amir Sadik and
                  Ioannis Antonoglou and Helen King and Dharshan
                  Kumaran and Daan Wierstra and Shane Legg and Demis
                  Hassabis},
  title =        {Human-Level Control Through Deep Reinforcement
                  Learning},
  journal =      {Nature},
  volume =       518,
  number =       7540,
  pages =        {529-533},
  year =         2015,
  doi =          {10.1038/nature14236},
  url =          {https://doi.org/10.1038/nature14236},
  DATE_ADDED =   {Wed Feb 14 11:50:21 2018},
}


@Book{,
  author =    {Richard S. Sutton and Andrew G. Barto},
  title =        {Reinforcement Learning: An Introduction, but is draft for 2nd edition, 2017},
  publisher =    {MIT Press},
  year =         {1998},
}

